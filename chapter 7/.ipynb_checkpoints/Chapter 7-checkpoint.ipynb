{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0843eeee",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787f4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "#usingmoons dataset that we used chapter 5\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcb175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#training a voting classifier using 3 classifiers\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf),('rf',rnd_clf),('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea3ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "#lets look at each classifier's accuracy\n",
    "#this is an example of hard voting\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32b438",
   "metadata": {},
   "source": [
    "what we notice here is that the voting classifier outperforms all other clasifiers, taht is due to the fact that the voting classifier uses all their outcomes to predict a more precise one; askinga crowd for answer and compiling them gives more information than asking one expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefd4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "#lets looka t an example of soft voting\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf),('rf',rnd_clf),('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf7fb6",
   "metadata": {},
   "source": [
    "soft voting gave us an even better performance of 91% !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3f65",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce9d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#example of bagging u√®sing SkLearn, we can use either bagging or pasting but most of the time we can just got for bagging as\n",
    "#default since it gives us overall better models, but if we have the time and cpu power required, we can go for \n",
    "#cross validation between the 2 and use the one with better results\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c4302",
   "metadata": {},
   "source": [
    "## Out of Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f210f0",
   "metadata": {},
   "source": [
    "Basically when using baging, some of the instances are not used (roughly 37% usually), so data taht is not part of the dataset used for the training is called \"out of bag\" data.\n",
    "We can use that data to test our model( by changing the value of \"oob_score\" to true ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a36af9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8906666666666667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we create a bagging classifier and turn oob_score on \n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500, \n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7e3af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we compare the oob_score to the accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61c9a4",
   "metadata": {},
   "source": [
    "turns out they are close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3588208a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40306122, 0.59693878],\n",
       "       [0.36416185, 0.63583815],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09473684, 0.90526316],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.01621622, 0.98378378],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.98019802, 0.01980198],\n",
       "       [0.74074074, 0.25925926],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.76966292, 0.23033708],\n",
       "       [0.88172043, 0.11827957],\n",
       "       [0.98387097, 0.01612903],\n",
       "       [0.06842105, 0.93157895],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98360656, 0.01639344],\n",
       "       [0.91428571, 0.08571429],\n",
       "       [0.9939759 , 0.0060241 ],\n",
       "       [0.01515152, 0.98484848],\n",
       "       [0.34871795, 0.65128205],\n",
       "       [0.84482759, 0.15517241],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97340426, 0.02659574],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62903226, 0.37096774],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.13636364, 0.86363636],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.34736842, 0.65263158],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21164021, 0.78835979],\n",
       "       [0.37560976, 0.62439024],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01485149, 0.98514851],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.97382199, 0.02617801],\n",
       "       [0.87431694, 0.12568306],\n",
       "       [0.94021739, 0.05978261],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05319149, 0.94680851],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [0.00584795, 0.99415205],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [0.77142857, 0.22857143],\n",
       "       [0.33160622, 0.66839378],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.66863905, 0.33136095],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83443709, 0.16556291],\n",
       "       [1.        , 0.        ],\n",
       "       [0.58426966, 0.41573034],\n",
       "       [0.08290155, 0.91709845],\n",
       "       [0.65934066, 0.34065934],\n",
       "       [0.91111111, 0.08888889],\n",
       "       [0.        , 1.        ],\n",
       "       [0.17613636, 0.82386364],\n",
       "       [0.91351351, 0.08648649],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07216495, 0.92783505],\n",
       "       [0.0295858 , 0.9704142 ],\n",
       "       [0.30939227, 0.69060773],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85714286, 0.14285714],\n",
       "       [0.01714286, 0.98285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21264368, 0.78735632],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.953125  , 0.046875  ],\n",
       "       [0.82222222, 0.17777778],\n",
       "       [0.01515152, 0.98484848],\n",
       "       [1.        , 0.        ],\n",
       "       [0.23958333, 0.76041667],\n",
       "       [0.61658031, 0.38341969],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04145078, 0.95854922],\n",
       "       [0.50520833, 0.49479167],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21393035, 0.78606965],\n",
       "       [0.51075269, 0.48924731],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00564972, 0.99435028],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.3015873 , 0.6984127 ],\n",
       "       [0.87570621, 0.12429379],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.80357143, 0.19642857],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99415205, 0.00584795],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98969072, 0.01030928],\n",
       "       [0.99375   , 0.00625   ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97714286, 0.02285714],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [0.01630435, 0.98369565],\n",
       "       [0.2513089 , 0.7486911 ],\n",
       "       [0.95375723, 0.04624277],\n",
       "       [0.24561404, 0.75438596],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70857143, 0.29142857],\n",
       "       [0.38674033, 0.61325967],\n",
       "       [0.4491018 , 0.5508982 ],\n",
       "       [0.82352941, 0.17647059],\n",
       "       [0.97126437, 0.02873563],\n",
       "       [0.04324324, 0.95675676],\n",
       "       [0.80225989, 0.19774011],\n",
       "       [0.0106383 , 0.9893617 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03626943, 0.96373057],\n",
       "       [0.97916667, 0.02083333],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0295858 , 0.9704142 ],\n",
       "       [0.0052356 , 0.9947644 ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94382022, 0.05617978],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.32972973, 0.67027027],\n",
       "       [0.29775281, 0.70224719],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.38202247, 0.61797753],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00487805, 0.99512195],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98429319, 0.01570681],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [0.68686869, 0.31313131],\n",
       "       [0.84924623, 0.15075377],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98994975, 0.01005025],\n",
       "       [0.9787234 , 0.0212766 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04494382, 0.95505618],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03278689, 0.96721311],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04918033, 0.95081967],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88826816, 0.11173184],\n",
       "       [0.75      , 0.25      ],\n",
       "       [0.59116022, 0.40883978],\n",
       "       [0.        , 1.        ],\n",
       "       [0.15469613, 0.84530387],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93939394, 0.06060606],\n",
       "       [0.97959184, 0.02040816],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00571429, 0.99428571],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43298969, 0.56701031],\n",
       "       [0.87861272, 0.12138728],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97727273, 0.02272727],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27472527, 0.72527473],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97126437, 0.02873563],\n",
       "       [0.83756345, 0.16243655],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07865169, 0.92134831],\n",
       "       [0.98974359, 0.01025641],\n",
       "       [0.02247191, 0.97752809],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02873563, 0.97126437],\n",
       "       [1.        , 0.        ],\n",
       "       [0.73743017, 0.26256983],\n",
       "       [0.        , 1.        ],\n",
       "       [0.88648649, 0.11351351],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [0.18181818, 0.81818182],\n",
       "       [0.19459459, 0.80540541],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22988506, 0.77011494],\n",
       "       [0.95854922, 0.04145078],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99456522, 0.00543478],\n",
       "       [0.        , 1.        ],\n",
       "       [0.52906977, 0.47093023],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07471264, 0.92528736],\n",
       "       [0.09340659, 0.90659341],\n",
       "       [0.98787879, 0.01212121],\n",
       "       [0.0273224 , 0.9726776 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33333333, 0.66666667],\n",
       "       [0.07453416, 0.92546584],\n",
       "       [0.47701149, 0.52298851],\n",
       "       [0.5801105 , 0.4198895 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.54819277, 0.45180723],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20512821, 0.79487179],\n",
       "       [0.80434783, 0.19565217],\n",
       "       [0.04790419, 0.95209581],\n",
       "       [1.        , 0.        ],\n",
       "       [0.8021978 , 0.1978022 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07486631, 0.92513369],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [0.92349727, 0.07650273],\n",
       "       [0.15533981, 0.84466019],\n",
       "       [0.97849462, 0.02150538],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.6031746 , 0.3968254 ],\n",
       "       [0.078125  , 0.921875  ],\n",
       "       [0.99      , 0.01      ],\n",
       "       [0.86740331, 0.13259669],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95857988, 0.04142012],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27160494, 0.72839506],\n",
       "       [0.98395722, 0.01604278],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00490196, 0.99509804],\n",
       "       [0.83957219, 0.16042781],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.73333333, 0.26666667],\n",
       "       [0.93922652, 0.06077348],\n",
       "       [1.        , 0.        ],\n",
       "       [0.68556701, 0.31443299],\n",
       "       [0.52763819, 0.47236181],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89411765, 0.10588235],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.81065089, 0.18934911],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7826087 , 0.2173913 ],\n",
       "       [0.0505618 , 0.9494382 ],\n",
       "       [0.54450262, 0.45549738],\n",
       "       [0.18579235, 0.81420765],\n",
       "       [0.        , 1.        ],\n",
       "       [0.859375  , 0.140625  ],\n",
       "       [0.86746988, 0.13253012],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98802395, 0.01197605],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02840909, 0.97159091],\n",
       "       [0.965     , 0.035     ],\n",
       "       [0.96335079, 0.03664921],\n",
       "       [1.        , 0.        ],\n",
       "       [0.53459119, 0.46540881],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97590361, 0.02409639],\n",
       "       [0.02272727, 0.97727273],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96721311, 0.03278689],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04469274, 0.95530726],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98780488, 0.01219512],\n",
       "       [0.0106383 , 0.9893617 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.11290323, 0.88709677],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01052632, 0.98947368],\n",
       "       [0.        , 1.        ],\n",
       "       [0.42079208, 0.57920792],\n",
       "       [0.06936416, 0.93063584],\n",
       "       [0.20903955, 0.79096045],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98360656, 0.01639344],\n",
       "       [0.20114943, 0.79885057],\n",
       "       [0.98492462, 0.01507538],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94086022, 0.05913978],\n",
       "       [0.43023256, 0.56976744],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98974359, 0.01025641],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.02673797, 0.97326203],\n",
       "       [0.98989899, 0.01010101],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01704545, 0.98295455],\n",
       "       [0.57222222, 0.42777778]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returning the class probabilities for each of each instance used \n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2755a6a",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328ea6a",
   "metadata": {},
   "source": [
    "BaggingClassifier supports sampling features as well. this is usefull when dealing with high dimensionnal inputs (such as images). This can be done by keeping all training instances (bootstrap=false and max_samples=1.0) but sampling features (bootstrap_features = true and max_features= a value smaller than 1.0) \n",
    "This trades a bit of bias for lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4fa84",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "878774a3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'max_leaf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6a8f444ced6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrnd_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mrnd_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'max_leaf'"
     ]
    }
   ],
   "source": [
    "#training a random forest classifier with 500 trees\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fa980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
