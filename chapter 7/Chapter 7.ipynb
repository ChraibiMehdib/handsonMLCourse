{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0843eeee",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787f4643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "#usingmoons dataset that we used chapter 5\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcb175c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#training a voting classifier using 3 classifiers\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf),('rf',rnd_clf),('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea3ab2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "#lets look at each classifier's accuracy\n",
    "#this is an example of hard voting\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32b438",
   "metadata": {},
   "source": [
    "what we notice here is that the voting classifier outperforms all other clasifiers, taht is due to the fact that the voting classifier uses all their outcomes to predict a more precise one; askinga crowd for answer and compiling them gives more information than asking one expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefd4920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.92\n"
     ]
    }
   ],
   "source": [
    "#lets looka t an example of soft voting\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf),('rf',rnd_clf),('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf7fb6",
   "metadata": {},
   "source": [
    "soft voting gave us an even better performance of 91% !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede3f65",
   "metadata": {},
   "source": [
    "## Bagging and Pasting in SkLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce9d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#example of bagging uèsing SkLearn, we can use either bagging or pasting but most of the time we can just got for bagging as\n",
    "#default since it gives us overall better models, but if we have the time and cpu power required, we can go for \n",
    "#cross validation between the 2 and use the one with better results\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c4302",
   "metadata": {},
   "source": [
    "## Out of Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73ccb0",
   "metadata": {},
   "source": [
    "Basically when using baging, some of the instances are not used (roughly 37% usually), so data taht is not part of the dataset used for the training is called \"out of bag\" data.\n",
    "We can use that data to test our model( by changing the value of \"oob_score\" to true ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a36af9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we create a bagging classifier and turn oob_score on \n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators = 500, \n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f47a716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we compare the oob_score to the accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae8834",
   "metadata": {},
   "source": [
    "turns out they are close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7608531d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37790698, 0.62209302],\n",
       "       [0.36931818, 0.63068182],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09278351, 0.90721649],\n",
       "       [0.38743455, 0.61256545],\n",
       "       [0.01142857, 0.98857143],\n",
       "       [0.98529412, 0.01470588],\n",
       "       [0.97252747, 0.02747253],\n",
       "       [0.79679144, 0.20320856],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.83510638, 0.16489362],\n",
       "       [0.84126984, 0.15873016],\n",
       "       [0.94972067, 0.05027933],\n",
       "       [0.07692308, 0.92307692],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.96363636, 0.03636364],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03888889, 0.96111111],\n",
       "       [0.32786885, 0.67213115],\n",
       "       [0.90804598, 0.09195402],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.65      , 0.35      ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16393443, 0.83606557],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01117318, 0.98882682],\n",
       "       [0.43604651, 0.56395349],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26436782, 0.73563218],\n",
       "       [0.30909091, 0.69090909],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.03550296, 0.96449704],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02325581, 0.97674419],\n",
       "       [0.99418605, 0.00581395],\n",
       "       [0.89772727, 0.10227273],\n",
       "       [0.97311828, 0.02688172],\n",
       "       [0.97894737, 0.02105263],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05405405, 0.94594595],\n",
       "       [0.98453608, 0.01546392],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01041667, 0.98958333],\n",
       "       [0.98275862, 0.01724138],\n",
       "       [0.81212121, 0.18787879],\n",
       "       [0.41477273, 0.58522727],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.74193548, 0.25806452],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.83333333, 0.16666667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.57303371, 0.42696629],\n",
       "       [0.14871795, 0.85128205],\n",
       "       [0.73099415, 0.26900585],\n",
       "       [0.89005236, 0.10994764],\n",
       "       [0.        , 1.        ],\n",
       "       [0.16847826, 0.83152174],\n",
       "       [0.85561497, 0.14438503],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09580838, 0.90419162],\n",
       "       [0.05649718, 0.94350282],\n",
       "       [0.36686391, 0.63313609],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01123596, 0.98876404],\n",
       "       [0.86931818, 0.13068182],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00492611, 0.99507389],\n",
       "       [0.        , 1.        ],\n",
       "       [0.2361809 , 0.7638191 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94413408, 0.05586592],\n",
       "       [0.77948718, 0.22051282],\n",
       "       [0.00510204, 0.99489796],\n",
       "       [1.        , 0.        ],\n",
       "       [0.20108696, 0.79891304],\n",
       "       [0.59668508, 0.40331492],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04255319, 0.95744681],\n",
       "       [0.54450262, 0.45549738],\n",
       "       [0.99447514, 0.00552486],\n",
       "       [0.02072539, 0.97927461],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.30348259, 0.69651741],\n",
       "       [0.53409091, 0.46590909],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01546392, 0.98453608],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [0.2972973 , 0.7027027 ],\n",
       "       [0.91237113, 0.08762887],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.81683168, 0.18316832],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00531915, 0.99468085],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9144385 , 0.0855615 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00537634, 0.99462366],\n",
       "       [0.22043011, 0.77956989],\n",
       "       [0.95348837, 0.04651163],\n",
       "       [0.28176796, 0.71823204],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68926554, 0.31073446],\n",
       "       [0.37142857, 0.62857143],\n",
       "       [0.40291262, 0.59708738],\n",
       "       [0.86363636, 0.13636364],\n",
       "       [0.94329897, 0.05670103],\n",
       "       [0.06521739, 0.93478261],\n",
       "       [0.80113636, 0.19886364],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [0.97777778, 0.02222222],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01005025, 0.98994975],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01052632, 0.98947368],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95054945, 0.04945055],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.44973545, 0.55026455],\n",
       "       [0.27932961, 0.72067039],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.30939227, 0.69060773],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.00502513, 0.99497487],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98994975, 0.01005025],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01587302, 0.98412698],\n",
       "       [0.64397906, 0.35602094],\n",
       "       [0.9375    , 0.0625    ],\n",
       "       [0.00526316, 0.99473684],\n",
       "       [0.99453552, 0.00546448],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07291667, 0.92708333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.04761905, 0.95238095],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04375   , 0.95625   ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92090395, 0.07909605],\n",
       "       [0.74752475, 0.25247525],\n",
       "       [0.55978261, 0.44021739],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14723926, 0.85276074],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94545455, 0.05454545],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40883978, 0.59116022],\n",
       "       [0.8708134 , 0.1291866 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99494949, 0.00505051],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97714286, 0.02285714],\n",
       "       [0.        , 1.        ],\n",
       "       [0.3368984 , 0.6631016 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97883598, 0.02116402],\n",
       "       [0.83060109, 0.16939891],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.08910891, 0.91089109],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02762431, 0.97237569],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04597701, 0.95402299],\n",
       "       [1.        , 0.        ],\n",
       "       [0.79428571, 0.20571429],\n",
       "       [0.        , 1.        ],\n",
       "       [0.85875706, 0.14124294],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.203125  , 0.796875  ],\n",
       "       [0.17894737, 0.82105263],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25789474, 0.74210526],\n",
       "       [0.95238095, 0.04761905],\n",
       "       [0.01129944, 0.98870056],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98429319, 0.01570681],\n",
       "       [0.        , 1.        ],\n",
       "       [0.48292683, 0.51707317],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11931818, 0.88068182],\n",
       "       [0.13297872, 0.86702128],\n",
       "       [0.9800995 , 0.0199005 ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [1.        , 0.        ],\n",
       "       [0.3902439 , 0.6097561 ],\n",
       "       [0.0982659 , 0.9017341 ],\n",
       "       [0.52717391, 0.47282609],\n",
       "       [0.59428571, 0.40571429],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.57142857, 0.42857143],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.21319797, 0.78680203],\n",
       "       [0.81122449, 0.18877551],\n",
       "       [0.10582011, 0.89417989],\n",
       "       [1.        , 0.        ],\n",
       "       [0.81407035, 0.18592965],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14814815, 0.85185185],\n",
       "       [0.0201005 , 0.9798995 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92      , 0.08      ],\n",
       "       [0.15025907, 0.84974093],\n",
       "       [0.97368421, 0.02631579],\n",
       "       [0.        , 1.        ],\n",
       "       [0.53296703, 0.46703297],\n",
       "       [0.11794872, 0.88205128],\n",
       "       [0.99453552, 0.00546448],\n",
       "       [0.81481481, 0.18518519],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96296296, 0.03703704],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.30113636, 0.69886364],\n",
       "       [0.99516908, 0.00483092],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.85245902, 0.14754098],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.71891892, 0.28108108],\n",
       "       [0.95897436, 0.04102564],\n",
       "       [1.        , 0.        ],\n",
       "       [0.69444444, 0.30555556],\n",
       "       [0.51595745, 0.48404255],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90673575, 0.09326425],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87777778, 0.12222222],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.72195122, 0.27804878],\n",
       "       [0.13333333, 0.86666667],\n",
       "       [0.49431818, 0.50568182],\n",
       "       [0.26630435, 0.73369565],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90502793, 0.09497207],\n",
       "       [0.84210526, 0.15789474],\n",
       "       [0.00546448, 0.99453552],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02209945, 0.97790055],\n",
       "       [0.96703297, 0.03296703],\n",
       "       [0.95209581, 0.04790419],\n",
       "       [1.        , 0.        ],\n",
       "       [0.46385542, 0.53614458],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97752809, 0.02247191],\n",
       "       [0.01675978, 0.98324022],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.9744898 , 0.0255102 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07514451, 0.92485549],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01630435, 0.98369565],\n",
       "       [1.        , 0.        ],\n",
       "       [0.12      , 0.88      ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00588235, 0.99411765],\n",
       "       [0.        , 1.        ],\n",
       "       [0.39247312, 0.60752688],\n",
       "       [0.07065217, 0.92934783],\n",
       "       [0.21546961, 0.78453039],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98882682, 0.01117318],\n",
       "       [0.27956989, 0.72043011],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.00581395, 0.99418605],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94594595, 0.05405405],\n",
       "       [0.37078652, 0.62921348],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99386503, 0.00613497],\n",
       "       [0.00505051, 0.99494949],\n",
       "       [0.04812834, 0.95187166],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02808989, 0.97191011],\n",
       "       [0.63793103, 0.36206897]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returning the class probabilities for each of each instance used \n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787bb61",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be359da",
   "metadata": {},
   "source": [
    "BaggingClassifier supports sampling features as well. this is usefull when dealing with high dimensionnal inputs (such as images). This can be done by keeping all training instances (bootstrap=false and max_samples=1.0) but sampling features (bootstrap_features = true and max_features= a value smaller than 1.0) \n",
    "This trades a bit of bias for lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7a4dd",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c383ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training a random forest classifier with 500 trees\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "424dddca",
   "metadata": {},
   "source": [
    "A RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier plus all the hyperparameters of a baggingClassifier (with a few exceptions).\n",
    "Random forest algorithm introduces extra randomness when growing trees; instead of searchiung for the very best feature when splitting a node, it searches for the best feature among random subset of features.\n",
    "The resulting algorithm trades higher bias for a lower variance yielding an overall better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1372e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a BaggingClassifier that is roughly the equivalent of the previous randomForestClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c865292",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd578fe7",
   "metadata": {},
   "source": [
    "extra-trees is actually called Extremely Randomized Trees; what it does is set a random number of features for splitting for each node.\n",
    "its api is the exact same as RandomForestClassifier(similarly, ExtraTreesRegressor has the same API as RandomForestRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51ded4",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db816aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.10136174131071805\n",
      "sepal width (cm) 0.02482022738446275\n",
      "petal length (cm) 0.44656247115207326\n",
      "petal width (cm) 0.42725556015274596\n"
     ]
    }
   ],
   "source": [
    "#RandomForest algorythme has a variable that stores the importance of each feature in the dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"],iris[\"target\"])\n",
    "#prints name and scores of each feature \n",
    "for name,score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5a3e3",
   "metadata": {},
   "source": [
    "## Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbae35",
   "metadata": {},
   "source": [
    "The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
    "cessor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a29b85",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b4152",
   "metadata": {},
   "source": [
    "adaboost works by training a base classifier (of our choosing) then uses it to make predictions ont he training set. \n",
    "The algorithme then increases the relative weight of missclassified training instances to focus harder on them and then trains a new model based on the new weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d380d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example of adaboost algorithme\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200, \n",
    "    algorithm=\"SAMME.R\", learning_rate = 0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696515cb",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f902a8",
   "metadata": {},
   "source": [
    "works the same way as AdaBoost except instead of adding predictors that correct old ones, it tries to fit the new model to the residual errors of the old predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a6e24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's look at a simple example\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "#lets build a regression tree first\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff67632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#then lets build a regression tree on the residual errors made by the first predictor\n",
    "\n",
    "y2= y-tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a34371b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2-tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80ce1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8,0.8]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d9f3040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06976324])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "006307ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all of that can also be done gradientBoostingRegressor in one line \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "721553b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=78)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is training a GBRT while using early stopping (in order to find the optimal number of trees (estimators))\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train,y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "         for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6456567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a25ec90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to implement it but this time instead of training all 120 and then looking back to the one the most fitting \n",
    "#as we train we look at the MSE of our model and if it doesnt improve 5 times in a row we stop the training and\n",
    "#take the one with the lowest MSE \n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "    if error_going_up == 5:\n",
    "        break # early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0143e59d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-5b2a9b3fa261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#an example of its uses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mxgb_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#another library we can use for our purposes is xgboost, think of it as a better optimized sklearn (with very similar library)\n",
    "#an example of its uses \n",
    "\n",
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "#it also automatically takes care of early stopping \n",
    "xgb_reg.fit(X_train, y_train,\n",
    " eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d15466",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76614d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
